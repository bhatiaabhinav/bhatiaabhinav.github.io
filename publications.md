## Publications


[RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$](#publications/BNZgenplan23)<br>
Bhatia, A., Nashed, SB., & Zilberstein, S. (2023). In _NeurIPS Workshop on Generalization in Planning_.<br>
<small>
TL;DR: Incorporating task-specific Q-value estimates as inputs to a meta-RL policy can lead to improved generalization and better performance over longer adaptation periods.
[PDF](files/BNZarxiv2024.pdf)
</small>


[Selecting the Partial State Abstractions of MDPs: A Metareasoning Approach with Deep Reinforcement Learning](#publications/NSBRZiros22)<br>
Nashed, S.B., Svegliato, J., Bhatia, A., Russell S., Zilberstein, S. (2022). In _IEEE/RSJ International Conference on Intelligent Robots and Systems_.<br>
<small>
TL;DR: Deep RL to select the most informative partial state abstractions for MDPs at runtime to optimize the time-dependent utility of the final solution. Good results on a variety of domains.
[PDF](files/NSBRZiros22.pdf)
</small>

[Adaptive Rollout Length for Model-Based RL Using Model-Free Deep RL](#publications/BTZarxiv22)<br>
Bhatia, A., Thomas, PS., & Zilberstein, S. (2022). In _arXiv preprint arXiv:2206.02380_.<br>
<small>
TL;DR: Meta-level deep RL to adapt the rollout-length in model-based RL non-myopically based on feedback from the learning process, such as accuracy of the model, learning progress and scarcity of samples.
[PDF](files/BTZarxiv22.pdf)
</small>


[Tuning the Hyperparameters of Anytime Planning: A Metareasoning Approach with Deep Reinforcement Learning](#publications/BSNZicaps22)<br>
Bhatia, A., Svegliato, J., Nashed, S. B., & Zilberstein, S. (2022). In _Proceedings of the International Conference on Automated Planning and Scheduling_.<br>
<small>
TL;DR: Deep RL to determine optimal stopping point _and_ hyperparameters of anytime algorithms at runtime to optimize _utility_ of the final solution. Good results on _Anytime A*_ search algorithm and _RRT*_ motion planning algorithm.
[PDF](files/BSNZicaps22.pdf)
</small>

[Tuning the Hyperparameters of Anytime Planning: A Deep Reinforcement Learning Approach](#publications/BSZhsdip2021)<br>
Bhatia, A., Svegliato, J., & Zilberstein, S. (2021). In _ICAPS Workshop on Heuristics and Search for Domain-independent Planning_.<br>
<small>
TL;DR: Deep RL to control hyperparameters of anytime algorithms at runtime to optimize quality of the final solution. Good results on _Anytime A*_ search algorithm.
[PDF](files/BSZhsdip2021.pdf)
</small>


[On the Benefits of Randomly Adjusting Anytime Weighted A*](#publications/BSZsocs21)<br>
Bhatia, A., Svegliato, J., & Zilberstein, S. (2021). In _Proceedings of the International Symposium on Combinatorial Search_.<br>
<small>
TL;DR: _Randomized Weighted A\*_ tunes the weight in _Anytime Weighted A\*_ randomly at runtime and outperforms every static weighted baseline.
[PDF](files/BSZsocs21.pdf)
</small>


[Resource Constrained Deep Reinforcement Learning](#publications/BVKicaps2019)<br>
Bhatia, A., Varakantham, P., & Kumar, A. (2019). In _Proceedings of the International Conference on Automated Planning and Scheduling_.<br>
<small>
TL;DR: Deep RL to optimize constrained resource allocation at city scale. Good results on realistic datasets.
[PDF](files/BVKicaps2019.pdf)
</small>
